{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model, save_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, concatenate, Conv2DTranspose\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Set up constants and directory paths\n",
    "# Dataset paths\n",
    "ROOT_DIR = \"../data/MSFD\"\n",
    "TRAIN_DIR = os.path.join(ROOT_DIR, \"1\")\n",
    "TEST_DIR = os.path.join(ROOT_DIR, \"2\")\n",
    "\n",
    "# Image directories\n",
    "TRAIN_IMG_DIR = os.path.join(TRAIN_DIR, \"img\")\n",
    "TRAIN_FACE_CROP_DIR = os.path.join(TRAIN_DIR, \"face_crop\")\n",
    "TRAIN_FACE_CROP_SEG_DIR = os.path.join(TRAIN_DIR, \"face_crop_segmentation\")\n",
    "TEST_IMG_DIR = os.path.join(TEST_DIR, \"img\")\n",
    "\n",
    "# Create directories for test results\n",
    "TEST_FACE_CROP_DIR = os.path.join(TEST_DIR, \"face_crop\")\n",
    "TEST_FACE_CROP_SEG_DIR = os.path.join(TEST_DIR, \"face_crop_segmentation\")\n",
    "PREDICTIONS_DIR = os.path.join(TEST_DIR, \"predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "os.makedirs(TEST_FACE_CROP_DIR, exist_ok=True)\n",
    "os.makedirs(TEST_FACE_CROP_SEG_DIR, exist_ok=True)\n",
    "os.makedirs(PREDICTIONS_DIR, exist_ok=True)\n",
    "\n",
    "# Model checkpoints directory\n",
    "CHECKPOINTS_DIR = os.path.join(ROOT_DIR, \"model_checkpoints\")\n",
    "os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n",
    "\n",
    "# Model parameters\n",
    "IMG_SIZE = 128  # Resize all images to 128x128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define IoU and Dice coefficient metrics\n",
    "def iou_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"Calculate IoU (Intersection over Union) coefficient.\"\"\"\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(tf.cast(y_pred > 0.5, tf.float32))\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    union = K.sum(y_true_f) + K.sum(y_pred_f) - intersection\n",
    "    return (intersection + smooth) / (union + smooth)\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"Calculate Dice coefficient.\"\"\"\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(tf.cast(y_pred > 0.5, tf.float32))\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "# Define loss functions\n",
    "def dice_loss(y_true, y_pred):\n",
    "    \"\"\"Dice loss function.\"\"\"\n",
    "    return 1 - dice_coef(y_true, y_pred)\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    \"\"\"Combined binary cross-entropy and dice loss.\"\"\"\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)(y_true, y_pred)\n",
    "    dice = dice_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load and examine the dataset\n",
    "# Load the CSV file containing bounding box information\n",
    "df = pd.read_csv(os.path.join(TRAIN_DIR, \"dataset.csv\"))\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check the distribution of masked and non-masked faces\n",
    "mask_distribution = df['with_mask'].value_counts(normalize=True) * 100\n",
    "print(\"\\nDistribution of masked vs non-masked faces:\")\n",
    "print(mask_distribution)\n",
    "\n",
    "# Visualize a sample image with its bounding box\n",
    "def visualize_sample(index=0):\n",
    "    row = df.iloc[index]\n",
    "    img_path = os.path.join(TRAIN_IMG_DIR, row['filename'])\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Draw bounding box\n",
    "    x1, y1, x2, y2 = row['x1'], row['y1'], row['x2'], row['y2']\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"File: {row['filename']}, Mask: {row['with_mask']}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Find corresponding cropped face and segmentation\n",
    "    face_id = row['filename'].split('.')[0]\n",
    "    face_crop_files = [f for f in os.listdir(TRAIN_FACE_CROP_DIR) if f.startswith(face_id)]\n",
    "    \n",
    "    if face_crop_files:\n",
    "        fig, axes = plt.subplots(len(face_crop_files), 2, figsize=(10, 4*len(face_crop_files)))\n",
    "        if len(face_crop_files) == 1:\n",
    "            axes = np.array([axes])\n",
    "        \n",
    "        for i, face_file in enumerate(face_crop_files):\n",
    "            # Display cropped face\n",
    "            face_img = cv2.imread(os.path.join(TRAIN_FACE_CROP_DIR, face_file))\n",
    "            face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n",
    "            axes[i, 0].imshow(face_img)\n",
    "            axes[i, 0].set_title(f\"Cropped Face: {face_file}\")\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            # Display segmentation mask\n",
    "            seg_img = cv2.imread(os.path.join(TRAIN_FACE_CROP_SEG_DIR, face_file), cv2.IMREAD_GRAYSCALE)\n",
    "            axes[i, 1].imshow(seg_img, cmap='gray')\n",
    "            axes[i, 1].set_title(f\"Segmentation: {face_file}\")\n",
    "            axes[i, 1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "visualize_sample(3)  # Visualize sample with index 3 (can change to see other samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Preprocess the data - Create dataset for training\n",
    "def get_face_crop_and_mask_paths():\n",
    "    \"\"\"Get paths of cropped faces and their corresponding masks.\"\"\"\n",
    "    face_files = sorted(os.listdir(TRAIN_FACE_CROP_DIR))\n",
    "    mask_files = sorted(os.listdir(TRAIN_FACE_CROP_SEG_DIR))\n",
    "    \n",
    "    # Ensure that both directories have the same files\n",
    "    common_files = set(face_files).intersection(set(mask_files))\n",
    "    print(f\"Found {len(common_files)} matching face crop and segmentation mask pairs.\")\n",
    "    \n",
    "    face_paths = [os.path.join(TRAIN_FACE_CROP_DIR, f) for f in face_files if f in common_files]\n",
    "    mask_paths = [os.path.join(TRAIN_FACE_CROP_SEG_DIR, f) for f in mask_files if f in common_files]\n",
    "    \n",
    "    return face_paths, mask_paths\n",
    "\n",
    "def preprocess_data():\n",
    "    \"\"\"Preprocess images and masks for the U-Net model.\"\"\"\n",
    "    face_paths, mask_paths = get_face_crop_and_mask_paths()\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    train_face_paths, val_face_paths, train_mask_paths, val_mask_paths = train_test_split(\n",
    "        face_paths, mask_paths, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(train_face_paths)}\")\n",
    "    print(f\"Validation samples: {len(val_face_paths)}\")\n",
    "    \n",
    "    return train_face_paths, val_face_paths, train_mask_paths, val_mask_paths\n",
    "\n",
    "# Get training and validation paths\n",
    "train_face_paths, val_face_paths, train_mask_paths, val_mask_paths = preprocess_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Data Generator for training\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Data generator for training and validation.\"\"\"\n",
    "    def __init__(self, face_paths, mask_paths, batch_size=16, img_size=128, augment=False, shuffle=True):\n",
    "        self.face_paths = face_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.augment = augment\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.face_paths))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of batches per epoch.\"\"\"\n",
    "        return int(np.ceil(len(self.face_paths) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Generate one batch of data.\"\"\"\n",
    "        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        batch_faces = [self.face_paths[i] for i in batch_indexes]\n",
    "        batch_masks = [self.mask_paths[i] for i in batch_indexes]\n",
    "        \n",
    "        X, y = self.__data_generation(batch_faces, batch_masks)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Update indexes after each epoch.\"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __data_generation(self, batch_faces, batch_masks):\n",
    "        \"\"\"Generate batches of augmented data.\"\"\"\n",
    "        X = np.empty((len(batch_faces), self.img_size, self.img_size, 3), dtype=np.float32)\n",
    "        y = np.empty((len(batch_faces), self.img_size, self.img_size, 1), dtype=np.float32)\n",
    "        \n",
    "        for i, (face_path, mask_path) in enumerate(zip(batch_faces, batch_masks)):\n",
    "            # Load and preprocess face image\n",
    "            face = cv2.imread(face_path)\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "            face = cv2.resize(face, (self.img_size, self.img_size))\n",
    "            \n",
    "            # Load and preprocess mask image\n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            mask = cv2.resize(mask, (self.img_size, self.img_size))\n",
    "            mask = np.expand_dims(mask, axis=-1)\n",
    "            \n",
    "            # Normalize images\n",
    "            face = face / 255.0\n",
    "            mask = mask / 255.0\n",
    "            \n",
    "            # Apply augmentation if enabled (can add more augmentation techniques)\n",
    "            if self.augment:\n",
    "                # Random horizontal flip\n",
    "                if np.random.random() > 0.5:\n",
    "                    face = np.fliplr(face)\n",
    "                    mask = np.fliplr(mask)\n",
    "                \n",
    "                # Random brightness adjustment\n",
    "                if np.random.random() > 0.5:\n",
    "                    factor = 0.2 + np.random.uniform(0, 0.8)\n",
    "                    face = face * factor\n",
    "                    face = np.clip(face, 0, 1.0)\n",
    "            \n",
    "            X[i,] = face\n",
    "            y[i,] = mask\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "# Create data generators\n",
    "train_generator = DataGenerator(\n",
    "    train_face_paths, \n",
    "    train_mask_paths,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    img_size=IMG_SIZE,\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "val_generator = DataGenerator(\n",
    "    val_face_paths, \n",
    "    val_mask_paths,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    img_size=IMG_SIZE,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "# Visualize a few training samples\n",
    "def visualize_batch_samples():\n",
    "    X_batch, y_batch = train_generator[0]\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 2, figsize=(10, 16))\n",
    "    for i in range(4):\n",
    "        # Display image\n",
    "        axes[i, 0].imshow(X_batch[i])\n",
    "        axes[i, 0].set_title(f\"Face Image {i+1}\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Display mask\n",
    "        axes[i, 1].imshow(y_batch[i].squeeze(), cmap='gray')\n",
    "        axes[i, 1].set_title(f\"Mask {i+1}\")\n",
    "        axes[i, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_batch_samples()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Define U-Net Model Architecture\n",
    "def build_unet_model(input_size=(IMG_SIZE, IMG_SIZE, 3)):\n",
    "    \"\"\"Build the U-Net model architecture.\"\"\"\n",
    "    # Input layer\n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    # Encoder (downsampling path)\n",
    "    # Block 1\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    # Block 2\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    # Block 3\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    # Block 4\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "    \n",
    "    # Bridge\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    \n",
    "    # Decoder (upsampling path)\n",
    "    # Block 6\n",
    "    up6 = Conv2DTranspose(512, 2, strides=(2, 2), padding='same')(drop5)\n",
    "    merge6 = concatenate([drop4, up6], axis=3)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(conv6)\n",
    "    \n",
    "    # Block 7\n",
    "    up7 = Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(conv6)\n",
    "    merge7 = concatenate([conv3, up7], axis=3)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(conv7)\n",
    "    \n",
    "    # Block 8\n",
    "    up8 = Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(conv7)\n",
    "    merge8 = concatenate([conv2, up8], axis=3)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(conv8)\n",
    "    \n",
    "    # Block 9\n",
    "    up9 = Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv8)\n",
    "    merge9 = concatenate([conv1, up9], axis=3)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(conv9)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Cell 8: Build and compile the model\n",
    "def compile_model(model):\n",
    "    \"\"\"Compile the model with optimizers and loss functions.\"\"\"\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=bce_dice_loss,\n",
    "        metrics=[dice_coef, iou_coef, 'accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build and compile the U-Net model\n",
    "model = build_unet_model(input_size=(IMG_SIZE, IMG_SIZE, 3))\n",
    "model = compile_model(model)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Define callbacks for training\n",
    "# Callbacks for model training\n",
    "callbacks = [\n",
    "    # Early stopping to prevent overfitting\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=10, \n",
    "        verbose=1, \n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate when a metric has stopped improving\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.5, \n",
    "        patience=5, \n",
    "        verbose=1, \n",
    "        min_lr=1e-6\n",
    "    ),\n",
    "    \n",
    "    # Save model checkpoints\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(CHECKPOINTS_DIR, 'mask_unet_model.h5'),\n",
    "        save_best_only=True,\n",
    "        monitor='val_dice_coef',\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Train the model\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the final model\n",
    "model.save(os.path.join(CHECKPOINTS_DIR, 'mask_unet_final_model.h5'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Evaluate model performance and visualize training metrics\n",
    "# Plot training history\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation metrics.\"\"\"\n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot dice coefficient\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history['dice_coef'], label='Training Dice')\n",
    "    plt.plot(history.history['val_dice_coef'], label='Validation Dice')\n",
    "    plt.title('Dice Coefficient')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Dice')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot IoU coefficient\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history.history['iou_coef'], label='Training IoU')\n",
    "    plt.plot(history.history['val_iou_coef'], label='Validation IoU')\n",
    "    plt.title('IoU Coefficient')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('IoU')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)\n",
    "\n",
    "# Cell 12: Evaluate model on validation set\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_dice, val_iou, val_acc = model.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Dice Coefficient: {val_dice:.4f}\")\n",
    "print(f\"Validation IoU: {val_iou:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Visualize predictions on validation samples\n",
    "def visualize_predictions(sample_count=5):\n",
    "    \"\"\"Visualize model predictions on validation samples.\"\"\"\n",
    "    # Get a batch from validation generator\n",
    "    X_val, y_val = next(iter(val_generator))\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Show sample_count predictions\n",
    "    n = min(sample_count, len(X_val))\n",
    "    fig, axes = plt.subplots(n, 3, figsize=(15, 5*n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Display original image\n",
    "        axes[i, 0].imshow(X_val[i])\n",
    "        axes[i, 0].set_title(\"Original Image\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Display ground truth mask\n",
    "        axes[i, 1].imshow(y_val[i].squeeze(), cmap='gray')\n",
    "        axes[i, 1].set_title(\"Ground Truth Mask\")\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Display predicted mask\n",
    "        axes[i, 2].imshow(y_pred[i].squeeze(), cmap='gray')\n",
    "        axes[i, 2].set_title(f\"Predicted Mask\\nDice: {dice_coef(y_val[i], y_pred[i]).numpy():.4f}\")\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Hyperparameter tuning (basic example)\n",
    "def hyperparameter_tuning():\n",
    "    \"\"\"Simple hyperparameter tuning for U-Net.\"\"\"\n",
    "    # Define hyperparameter combinations to try\n",
    "    learning_rates = [1e-3, 1e-4]\n",
    "    batch_sizes = [8, 16]\n",
    "    \n",
    "    # Results dictionary\n",
    "    results = []\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        for bs in batch_sizes:\n",
    "            print(f\"\\nTesting LR={lr}, Batch Size={bs}\")\n",
    "            \n",
    "            # Create generators with current batch size\n",
    "            train_gen = DataGenerator(\n",
    "                train_face_paths[:100],  # Use subset for quick tuning\n",
    "                train_mask_paths[:100],\n",
    "                batch_size=bs,\n",
    "                img_size=IMG_SIZE,\n",
    "                augment=True\n",
    "            )\n",
    "            \n",
    "            val_gen = DataGenerator(\n",
    "                val_face_paths[:50],  # Use subset for quick tuning\n",
    "                val_mask_paths[:50],\n",
    "                batch_size=bs,\n",
    "                img_size=IMG_SIZE,\n",
    "                augment=False\n",
    "            )\n",
    "            \n",
    "            # Build and compile model with current learning rate\n",
    "            model = build_unet_model()\n",
    "            model.compile(\n",
    "                optimizer=Adam(learning_rate=lr),\n",
    "                loss=bce_dice_loss,\n",
    "                metrics=[dice_coef, iou_coef]\n",
    "            )\n",
    "            \n",
    "            # Train for a few epochs\n",
    "            history = model.fit(\n",
    "                train_gen,\n",
    "                epochs=5,  # Few epochs for quick evaluation\n",
    "                validation_data=val_gen,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Get final validation metrics\n",
    "            val_loss = history.history['val_loss'][-1]\n",
    "            val_dice = history.history['val_dice_coef'][-1]\n",
    "            val_iou = history.history['val_iou_coef'][-1]\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'learning_rate': lr,\n",
    "                'batch_size': bs,\n",
    "                'val_loss': val_loss,\n",
    "                'val_dice': val_dice,\n",
    "                'val_iou': val_iou\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame and find best combination\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nHyperparameter Tuning Results:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    best_combo = results_df.loc[results_df['val_dice'].idxmax()]\n",
    "    print(f\"\\nBest Hyperparameters:\\nLearning Rate: {best_combo['learning_rate']}\\nBatch Size: {best_combo['batch_size']}\")\n",
    "    print(f\"Best Validation Dice: {best_combo['val_dice']:.4f}\")\n",
    "    \n",
    "    return best_combo['learning_rate'], best_combo['batch_size']\n",
    "\n",
    "# Comment out if you don't want to run hyperparameter tuning\n",
    "# best_lr, best_bs = hyperparameter_tuning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
